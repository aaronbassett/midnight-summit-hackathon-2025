openapi: 3.0.3
info:
  title: Bandaid LLM Security Proxy API
  description: |
    Transparent security proxy for LLM applications. Provides OpenAI-compatible endpoints
    with integrated threat detection, data leak prevention, and self-learning capabilities.

    **Key Features:**
    - Multi-layer threat detection (prompt injection, jailbreak, PII, financial secrets)
    - Tiered confidence thresholds (high=block, medium=warn, low=allow)
    - Self-learning attack pattern recognition
    - Streaming support for real-time responses
    - Multiple LLM provider support via LiteLLM

  version: 1.0.0
  contact:
    name: Bandaid Support
    url: https://github.com/yourorg/bandaid
  license:
    name: MIT

servers:
  - url: http://localhost:8000
    description: Local proxy server (default)

tags:
  - name: LLM Proxy
    description: OpenAI-compatible LLM endpoints (proxied)
  - name: Health
    description: Server health and status

paths:
  /v1/chat/completions:
    post:
      tags:
        - LLM Proxy
      summary: Create chat completion
      description: |
        OpenAI-compatible chat completions endpoint. Requests are validated for threats
        before forwarding to the configured LLM provider.

        **Security Flow:**
        1. Request validated by NER (PII/secrets), Guard (policy), and learned patterns
        2. High-confidence threats blocked with 403
        3. Medium-confidence threats logged but allowed
        4. Responses scanned for data leaks (logged, not blocked)

      operationId: createChatCompletion
      security:
        - BearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatCompletionRequest'
            examples:
              simple:
                summary: Simple chat completion
                value:
                  model: "gpt-4"
                  messages:
                    - role: "user"
                      content: "What is the capital of France?"
              streaming:
                summary: Streaming response
                value:
                  model: "gpt-4"
                  messages:
                    - role: "user"
                      content: "Tell me a story"
                  stream: true
      responses:
        '200':
          description: Chat completion successful
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionResponse'
            text/event-stream:
              schema:
                type: string
                description: Server-sent events for streaming responses
              example: |
                data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

                data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4","choices":[{"index":0,"delta":{"content":"The"},"finish_reason":null}]}

                data: [DONE]
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthorized'
        '403':
          $ref: '#/components/responses/ThreatDetected'
        '500':
          $ref: '#/components/responses/InternalServerError'
        '502':
          $ref: '#/components/responses/UpstreamError'

  /v1/completions:
    post:
      tags:
        - LLM Proxy
      summary: Create completion
      description: |
        OpenAI-compatible completions endpoint (legacy). Same security validation as
        chat completions.
      operationId: createCompletion
      security:
        - BearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionRequest'
      responses:
        '200':
          description: Completion successful
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CompletionResponse'
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthorized'
        '403':
          $ref: '#/components/responses/ThreatDetected'
        '500':
          $ref: '#/components/responses/InternalServerError'

  /v1/embeddings:
    post:
      tags:
        - LLM Proxy
      summary: Create embeddings
      description: |
        OpenAI-compatible embeddings endpoint. Security validation applies but is
        typically lightweight for embeddings (no Guard validation).
      operationId: createEmbeddings
      security:
        - BearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/EmbeddingRequest'
      responses:
        '200':
          description: Embeddings created successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/EmbeddingResponse'
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthorized'
        '403':
          $ref: '#/components/responses/ThreatDetected'
        '500':
          $ref: '#/components/responses/InternalServerError'

  /health:
    get:
      tags:
        - Health
      summary: Health check
      description: |
        Returns server health status including model loading status and provider connectivity.
      operationId: healthCheck
      responses:
        '200':
          description: Server is healthy
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthResponse'
        '503':
          description: Server is unhealthy
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthResponse'

  /metrics:
    get:
      tags:
        - Health
      summary: Prometheus metrics
      description: |
        Returns Prometheus-style metrics for monitoring (optional feature).
      operationId: getMetrics
      responses:
        '200':
          description: Metrics retrieved successfully
          content:
            text/plain:
              schema:
                type: string
                example: |
                  # HELP bandaid_requests_total Total number of requests
                  # TYPE bandaid_requests_total counter
                  bandaid_requests_total{status="blocked"} 42
                  bandaid_requests_total{status="allowed"} 1337

components:
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
      description: |
        LLM provider API key (e.g., OpenAI API key). The proxy forwards this to the
        configured provider.

  schemas:
    ChatCompletionRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: Model identifier (forwarded to provider)
          example: "gpt-4"
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatMessage'
          minItems: 1
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
        n:
          type: integer
          minimum: 1
          default: 1
        stream:
          type: boolean
          default: false
        max_tokens:
          type: integer
          minimum: 1
        presence_penalty:
          type: number
          minimum: -2
          maximum: 2
          default: 0
        frequency_penalty:
          type: number
          minimum: -2
          maximum: 2
          default: 0
        user:
          type: string
          description: End-user identifier (for tracking)

    ChatMessage:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum: [system, user, assistant, function]
        content:
          type: string
        name:
          type: string
        function_call:
          type: object

    ChatCompletionResponse:
      type: object
      required:
        - id
        - object
        - created
        - model
        - choices
      properties:
        id:
          type: string
          example: "chatcmpl-123"
        object:
          type: string
          enum: [chat.completion]
        created:
          type: integer
          description: Unix timestamp
        model:
          type: string
        choices:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionChoice'
        usage:
          $ref: '#/components/schemas/Usage'

    ChatCompletionChoice:
      type: object
      required:
        - index
        - message
        - finish_reason
      properties:
        index:
          type: integer
        message:
          $ref: '#/components/schemas/ChatMessage'
        finish_reason:
          type: string
          enum: [stop, length, function_call, content_filter, null]
          nullable: true

    CompletionRequest:
      type: object
      required:
        - model
        - prompt
      properties:
        model:
          type: string
        prompt:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
        max_tokens:
          type: integer
          minimum: 1
        temperature:
          type: number
          minimum: 0
          maximum: 2
        stream:
          type: boolean
          default: false

    CompletionResponse:
      type: object
      required:
        - id
        - object
        - created
        - model
        - choices
      properties:
        id:
          type: string
        object:
          type: string
          enum: [text_completion]
        created:
          type: integer
        model:
          type: string
        choices:
          type: array
          items:
            $ref: '#/components/schemas/CompletionChoice'
        usage:
          $ref: '#/components/schemas/Usage'

    CompletionChoice:
      type: object
      required:
        - text
        - index
        - finish_reason
      properties:
        text:
          type: string
        index:
          type: integer
        finish_reason:
          type: string

    EmbeddingRequest:
      type: object
      required:
        - model
        - input
      properties:
        model:
          type: string
          example: "text-embedding-ada-002"
        input:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
        user:
          type: string

    EmbeddingResponse:
      type: object
      required:
        - object
        - data
        - model
        - usage
      properties:
        object:
          type: string
          enum: [list]
        data:
          type: array
          items:
            $ref: '#/components/schemas/Embedding'
        model:
          type: string
        usage:
          $ref: '#/components/schemas/Usage'

    Embedding:
      type: object
      required:
        - object
        - embedding
        - index
      properties:
        object:
          type: string
          enum: [embedding]
        embedding:
          type: array
          items:
            type: number
        index:
          type: integer

    Usage:
      type: object
      required:
        - prompt_tokens
        - total_tokens
      properties:
        prompt_tokens:
          type: integer
        completion_tokens:
          type: integer
        total_tokens:
          type: integer

    HealthResponse:
      type: object
      required:
        - status
        - timestamp
      properties:
        status:
          type: string
          enum: [healthy, degraded, unhealthy]
        timestamp:
          type: string
          format: date-time
        models:
          type: object
          properties:
            ner_loaded:
              type: boolean
            guard_loaded:
              type: boolean
            embeddings_loaded:
              type: boolean
        providers:
          type: object
          additionalProperties:
            type: object
            properties:
              reachable:
                type: boolean
              last_check:
                type: string
                format: date-time
        storage:
          type: object
          properties:
            sqlite_ok:
              type: boolean
            chromadb_ok:
              type: boolean

    ErrorResponse:
      type: object
      required:
        - error
      properties:
        error:
          type: object
          required:
            - message
            - type
          properties:
            message:
              type: string
            type:
              type: string
            code:
              type: string

    ThreatDetectedResponse:
      type: object
      required:
        - error
        - threat_type
        - confidence
        - request_id
      properties:
        error:
          type: string
          enum: [threat_detected]
          example: "threat_detected"
        threat_type:
          type: string
          enum: [prompt_injection, jailbreak, pii, financial_secret, toxic_content]
        confidence:
          type: number
          minimum: 0
          maximum: 1
          description: Detection confidence score
        request_id:
          type: string
          format: uuid
          description: Request ID for tracking/audit
        details:
          type: string
          description: Human-readable explanation (no sensitive data)
          example: "Request contains potential prompt injection patterns"

  responses:
    BadRequest:
      description: Invalid request format
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ErrorResponse'
          example:
            error:
              message: "Invalid request: missing required field 'model'"
              type: "invalid_request_error"
              code: "missing_required_parameter"

    Unauthorized:
      description: Missing or invalid API key
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ErrorResponse'
          example:
            error:
              message: "Invalid API key provided"
              type: "invalid_request_error"
              code: "invalid_api_key"

    ThreatDetected:
      description: Request blocked due to detected threat
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ThreatDetectedResponse'
          examples:
            prompt_injection:
              summary: Prompt injection detected
              value:
                error: "threat_detected"
                threat_type: "prompt_injection"
                confidence: 0.95
                request_id: "550e8400-e29b-41d4-a716-446655440000"
                details: "Request contains potential prompt injection patterns"
            pii_detected:
              summary: PII detected in request
              value:
                error: "threat_detected"
                threat_type: "pii"
                confidence: 0.98
                request_id: "650e8400-e29b-41d4-a716-446655440001"
                details: "Request contains personally identifiable information"

    InternalServerError:
      description: Internal server error
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ErrorResponse'
          example:
            error:
              message: "An internal error occurred during processing"
              type: "internal_error"
              code: "internal_server_error"

    UpstreamError:
      description: Error from upstream LLM provider
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ErrorResponse'
          example:
            error:
              message: "Upstream provider error: Service temporarily unavailable"
              type: "upstream_error"
              code: "provider_unavailable"
